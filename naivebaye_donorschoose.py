# -*- coding: utf-8 -*-
"""NaiveBaye-DonorsChoose.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W4vmh2KdcQZotOt-WyVfhyZN6HS_kYEY

**DonorsChoose**

DonorsChoose is a nonprofit that addresses the education funding gap through crowdfunding. Since 2000, they have facilitated $970 million in donations to 40 million students in the United States.

Features selected are listed below:

- essay

categorical features

- teacher_prefix
- project_grade_category
- school_state
- clean_categories
- clean_subcategories

numerical features

- price
- teacher_number_of_previously_posted_projects

##Loading Data
"""

import pandas
data = pandas.read_csv('/content/drive/MyDrive/preprocessed_data.csv')
data.tail(10)

"""# Split the dataset """

# Split the dataset 

from sklearn.model_selection import train_test_split

data = data[0:5000];


X_train,X_test,Y_train,Y_test = train_test_split(data, data['project_is_approved'],test_size=0.33, random_state=42,stratify=data['project_is_approved'])
X_train, X_cv, y_train, y_cv = train_test_split(X_train, Y_train, test_size=0.33, stratify=Y_train);

print(X_test['teacher_prefix'].unique())

X_train.drop(['project_is_approved'], axis=1, inplace=True)
X_test.drop(['project_is_approved'], axis=1, inplace=True)
X_cv.drop(['project_is_approved'], axis=1, inplace=True)

print(data.columns)
print(X_train.shape, X_cv.shape, y_train.shape, y_cv.shape, X_test.shape,Y_test.shape)

"""##Make Data Model Ready: encoding essay"""

from tqdm import tqdm
from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer

countVec = CountVectorizer(min_df=10,ngram_range=(1,2), max_features=2000);

# fit only on train data
countVec.fit(X_train['essay'].values);

X_train_essay_bow = countVec.transform(X_train['essay'].values)
X_cv_essay_bow = countVec.transform(X_cv['essay'].values)
X_test_essay_bow = countVec.transform(X_test['essay'].values)
print(X_train_essay_bow.shape)

vectorizer= TfidfVectorizer(ngram_range = (1,3));

vectorizer.fit(X_train['essay'].values);
X_train_tf_idf_essay = vectorizer.transform(X_train['essay'].values);
X_cv_tf_idf_essay = vectorizer.transform(X_cv['essay'].values);
X_test_tf_idf_essay = vectorizer.transform(X_test['essay'].values);

"""##Make Data Model Ready: encoding numerical, categorical features"""

# print(data.columns)
vect = CountVectorizer();

vect.fit(X_train['school_state'].values);
X_train_school_state_ohe = vect.transform(X_train['school_state'].values);
X_cv_school_state_ohe = vect.transform(X_cv['school_state'].values);
X_test_school_state_ohe = vect.transform(X_test['school_state'].values);
print(vect.get_feature_names());

vect = CountVectorizer();
vect.fit(X_train['teacher_prefix'].values);
X_train_teacher_prefix_ohe = vect.transform(X_train['teacher_prefix'].values);
X_cv_teacher_prefix_ohe = vect.transform(X_cv['teacher_prefix'].values);
X_test_teacher_prefix_ohe = vect.transform(X_test['teacher_prefix'].values);
print(vect.get_feature_names());

vect = CountVectorizer();
vect.fit(X_train['project_grade_category'].values);
X_train_project_grade_category_ohe = vect.transform(X_train['project_grade_category'].values);
X_cv_project_grade_category_ohe = vect.transform(X_cv['project_grade_category'].values);
X_test_project_grade_category_ohe = vect.transform(X_test['project_grade_category'].values);
print(vect.get_feature_names());

vect = CountVectorizer();
vect.fit(X_train['clean_categories'].values);
X_train_clean_categories_ohe = vect.transform(X_train['clean_categories'].values);
X_cv_clean_categories_ohe = vect.transform(X_cv['clean_categories'].values);
X_test_clean_categories_ohe = vect.transform(X_test['clean_categories'].values);
print(vect.get_feature_names());

vect = CountVectorizer();
vect.fit(X_train['clean_subcategories'].values);
X_train_clean_subcategories_ohe= vect.transform(X_train['clean_subcategories'].values);
X_cv_clean_subcategories_ohe = vect.transform(X_cv['clean_subcategories'].values);
X_test_clean_subcategories_ohe= vect.transform(X_test['clean_subcategories'].values);

# Normailzer on numeric values
from sklearn.preprocessing import Normalizer
norm = Normalizer();

norm.fit(X_train['price'].values.reshape(1, -1).reshape(-1,1));
X_train_price_norm = norm.transform(X_train['price'].values.reshape(1, -1).reshape(-1,1));
X_cv_price_norm = norm.fit_transform(X_cv['price'].values.reshape(1,-1).reshape(-1,1));
X_test_price_norm = norm.fit_transform(X_test['price'].values.reshape(1,-1).reshape(-1,1));

norm = Normalizer();
norm.fit(X_train['teacher_number_of_previously_posted_projects'].values.reshape(1,-1).reshape(-1,1));
X_train_teacher_number_of_previously_posted_projects_Norm = norm.fit_transform(X_train['teacher_number_of_previously_posted_projects'].values.reshape(1,-1).reshape(-1,1));
X_cv_teacher_number_of_previously_posted_projects_Norm = norm.fit_transform(X_cv['teacher_number_of_previously_posted_projects'].values.reshape(1,-1).reshape(-1,1));
X_test_teacher_number_of_previously_posted_projects_Norm = norm.fit_transform(X_test['teacher_number_of_previously_posted_projects'].values.reshape(1,-1).reshape(-1,1));

"""### Perform Hyperparameter Tuning"""

# Perform Hyperparameter Tuning.

from sklearn.model_selection import RandomizedSearchCV
from sklearn.naive_bayes import GaussianNB
from scipy.sparse import hstack
import matplotlib.pyplot as plt 

naiveCLf = GaussianNB();
parameters = {'var_smoothing':[0.1, 10, 100, 51, .004]};

print(naiveCLf.get_params().keys())

X_tr = hstack((X_train_essay_bow,X_train_price_norm,X_train_school_state_ohe,X_train_project_grade_category_ohe,
               X_train_clean_categories_ohe,X_train_clean_subcategories_ohe,X_train_teacher_prefix_ohe,X_train_teacher_number_of_previously_posted_projects_Norm )).tocsr()
X_cr = hstack((X_cv_essay_bow,X_cv_price_norm,X_cv_school_state_ohe,X_cv_project_grade_category_ohe,
               X_cv_clean_categories_ohe,X_cv_clean_subcategories_ohe,X_cv_teacher_prefix_ohe,X_cv_teacher_number_of_previously_posted_projects_Norm )).tocsr()
X_tst =  hstack((X_test_essay_bow,X_test_price_norm,X_test_school_state_ohe,X_test_project_grade_category_ohe,
               X_test_clean_categories_ohe,X_test_clean_subcategories_ohe,X_test_teacher_prefix_ohe,X_test_teacher_number_of_previously_posted_projects_Norm )).tocsr()
clf = RandomizedSearchCV(naiveCLf, parameters , verbose =1 ,cv =3, scoring ='roc_auc', return_train_score= True );


print("Final Data matrix")
print(X_tr.shape, y_train.shape)
print(X_cr.shape, y_cv.shape)
print(X_tst.shape, Y_test.shape)

clf.fit(X_tr.toarray(),y_train);

print(clf.cv_results_);


train_auc= clf.cv_results_['mean_train_score']
train_auc_std= clf.cv_results_['std_train_score']
cv_auc = clf.cv_results_['mean_test_score'] 
cv_auc_std= clf.cv_results_['std_test_score']

print(clf.best_params_);

"""##Plotting the training and the CV AUC scores, for different values of 'alpha', using a 2D line plot"""

k=sorted(parameters['var_smoothing'],reverse=False);
plt.plot(k,train_auc,label='TRAIN AUC ');
plt.plot(k,cv_auc,label='CV AUC ');

plt.scatter(k, train_auc, label='Train AUC points')
plt.scatter(k, cv_auc, label='CV AUC points')

plt.legend();
plt.xlabel("hyperparameter")
plt.ylabel("AUC")
plt.title("ERROR PLOTS")
plt.grid()
plt.show()

"""##Obtain the optimal value for 'alpha' and using the obtained optimal 'alpha' value, fit a multinomial naive bayes model, on the train data,"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import roc_auc_score ,auc,accuracy_score
from sklearn.metrics import roc_curve, auc
import pandas as pd
from scipy.sparse import vstack 
import numpy as np

#  so after hyperparameter training , the best param is 0.004 
best_param = 0.004;


clf = MultinomialNB(alpha = best_param);
# df = pd.DataFrame.sparse.from_spmatrix(X_tr.toarray,X_cr);
X_train_final = vstack((X_tr,X_cr));

Y_train_final = pd.concat([y_train, y_cv]);


clf.fit(X_train_final,Y_train_final);
Yred = clf.predict(X_tst)


YTrain_Predicted_Proba = clf.predict_proba(X_train_final)[:,1];
YTest_Predicted_Proba = clf.predict_proba(X_tst)[:,1];



train_fpr, train_tpr, tr_thresholds   = roc_curve(Y_train_final,YTrain_Predicted_Proba);
test_fpr, test_tpr, te_thresholds = roc_curve(Y_test,YTest_Predicted_Proba);


print(str(auc(train_fpr, train_tpr)));
print(str(auc(test_fpr, test_tpr)));

"""### Plot the ROC-AUC curves using the probability predictions made on train and test data."""

plt.plot(train_fpr,train_tpr,label='train AUC curve ='+ str(auc(train_fpr, train_tpr)));
plt.plot(test_fpr,test_tpr,label='test AUC curve = '+str(auc(test_fpr, test_tpr)));
plt.legend()
plt.xlabel("False Positive Rate(FPR)")
plt.ylabel("True Positive Rate(TPR)")
plt.show()